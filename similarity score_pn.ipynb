{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1adfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fishrak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fishrak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "import os\n",
    "import docx2txt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30d74ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg',disable = ['ner', 'parser'])\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82534fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbc_doc\\business\\001.docx\n",
      "bbc_doc\\business\\002.docx\n",
      "bbc_doc\\business\\003.docx\n",
      "bbc_doc\\business\\004.docx\n",
      "bbc_doc\\business\\005.docx\n",
      "bbc_doc\\entertainment\\001.docx\n",
      "bbc_doc\\entertainment\\002.docx\n",
      "bbc_doc\\entertainment\\003.docx\n",
      "bbc_doc\\entertainment\\004.docx\n",
      "bbc_doc\\entertainment\\005.docx\n",
      "bbc_doc\\politics\\001.docx\n",
      "bbc_doc\\politics\\002.docx\n",
      "bbc_doc\\politics\\003.docx\n",
      "bbc_doc\\politics\\004.docx\n",
      "bbc_doc\\politics\\005.docx\n",
      "bbc_doc\\sports\\001.docx\n",
      "bbc_doc\\sports\\002.docx\n",
      "bbc_doc\\sports\\003.docx\n",
      "bbc_doc\\sports\\004.docx\n",
      "bbc_doc\\sports\\005.docx\n",
      "bbc_doc\\technology\\001.docx\n",
      "bbc_doc\\technology\\002.docx\n",
      "bbc_doc\\technology\\003.docx\n",
      "bbc_doc\\technology\\004.docx\n",
      "bbc_doc\\technology\\005.docx\n"
     ]
    }
   ],
   "source": [
    "def all_file_paths(master_directory):\n",
    "    path_l = []\n",
    "    for root, dirs, files in os.walk(master_directory):\n",
    "        if files:\n",
    "            for i in files:\n",
    "                path = \"{}\\{}\".format(root,i)\n",
    "                if path == 'bbc_doc\\desktop.ini':\n",
    "                    continue\n",
    "                path_l.append(path)\n",
    "                \n",
    "    \n",
    "    return(path_l)\n",
    "\n",
    "all_path = all_file_paths('bbc_doc')\n",
    "\n",
    "for i in all_path:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cd9d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = re.sub('[\\W]+', '', text.lower())\n",
    "    return text\n",
    "\n",
    "\n",
    "def findline(word,doc_list): #Added doc_list as var\n",
    "    line_num = []\n",
    "    line_list = []\n",
    "    for i in range(len(doc_list)):\n",
    "        if word in doc_list[i]:\n",
    "            line_num.append(i+1)\n",
    "            line_list.append(doc_list[i])\n",
    "\n",
    "    return line_num, line_list\n",
    "\n",
    "def word_occurences(text, tokens_str, query):\n",
    "    \n",
    "    wordfreq = []\n",
    "    for w in tokens_str:\n",
    "        wordfreq.append(tokens_str.count(w))\n",
    "\n",
    "\n",
    "    for i in list(zip(tokens_str, wordfreq)):\n",
    "        if query == i[0]:\n",
    "            word_occurences = (i[0], i[1])\n",
    "            break\n",
    "\n",
    "    line = 0\n",
    "    word_not_found = []\n",
    "\n",
    "    for word in text:\n",
    "        if word == '\\n':\n",
    "            line += 1\n",
    "\n",
    "    doc_list = text.lower().splitlines()\n",
    "    doc_list = [i for i in doc_list if i]\n",
    "    \n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4deb01d7",
   "metadata": {},
   "outputs": [],
   "source": [
    " #paragraph_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5606f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_word(path,query,score):\n",
    " \n",
    "#FI\n",
    "    text = docx2txt.process(path)\n",
    "    string_txt = str(text)\n",
    "\n",
    "    tokens = nlp(text)\n",
    "    lemma_list = []\n",
    "    for token in tokens:\n",
    "        if token.is_stop is False:\n",
    "            token_preprocessed = preprocessor(token.text)\n",
    "            if token_preprocessed != '':\n",
    "                 lemma_list.append(nlp(token_preprocessed))\n",
    "   \n",
    "\n",
    "    tokens_str = text.lower()\n",
    "    tokens_str = word_tokenize(tokens_str)\n",
    "    tokens_str = [i for i in tokens_str if not i in stop_words]    \n",
    "#FI    \n",
    "    \n",
    "    query = query\n",
    "    \n",
    "    l_q = len(nlp(query))\n",
    "    \n",
    "    if l_q >= 2:\n",
    "        \n",
    "        bi_g = []\n",
    "        c=0\n",
    "        while c < len(lemma_list)-1:\n",
    "\n",
    "            bi_g.append(nlp(lemma_list[c].text+\" \"+lemma_list[c+1].text))\n",
    "            c = c+1\n",
    "\n",
    "\n",
    "        lemma_list = lemma_list+bi_g\n",
    "    \n",
    "#PN \n",
    "    doc_list = word_occurences(text, lemma_list, query)\n",
    "    \n",
    "    string_list = []\n",
    "    paragraph_list = []\n",
    "    paragraph_matrix_list = []\n",
    "    \n",
    "    string_list.append(string_txt)\n",
    "    \n",
    "    string_list = string_list[0].splitlines()\n",
    "    \n",
    "    for word in string_list:\n",
    "        if word != '':\n",
    "            paragraph_list.append(word)\n",
    "    \n",
    "    #print(paragraph_list)\n",
    "#PN\n",
    "\n",
    "\n",
    "#FI & PN\n",
    "    key = nlp(query)\n",
    "    \n",
    "    sim_score = []\n",
    "    words = []\n",
    "    word_count = []\n",
    "    #paragraph_word_set = set()\n",
    "   # paragraph_sentences = []\n",
    "\n",
    "    pg_list = []\n",
    "    line_list_words = []\n",
    "\n",
    "    for i in lemma_list:\n",
    "        s = key.similarity(i)\n",
    "\n",
    "        if s > score:\n",
    "            words.append(i)\n",
    "                \n",
    "            # PN\n",
    "            sim_score.append(s)\n",
    "\n",
    "            word_found_list = []\n",
    "            word_found_list.append(str(i.text))\n",
    "            word_found_lines, line_list = findline(word_found_list[0],doc_list)\n",
    "            #print(line_list)\n",
    "            l_line_list = []\n",
    "            l_line_list.append(tuple(line_list))\n",
    "            paragraph_lines = list(line_list)\n",
    "            #paragraph_lines = [ele for ele in paragraph_lines if ele != []]\n",
    "            word_count.append(len(word_found_lines))\n",
    "            \n",
    "            pg_list.append(tuple(word_found_lines))\n",
    "            line_list_words.append(tuple(line_list))\n",
    "    \n",
    "    matrix = pd.DataFrame({'Similarity Score': sim_score,'Paragraph Numbers': pg_list, 'Count': word_count, 'Paragraphs': line_list_words, 'Path': path}, index = words)\n",
    "\n",
    "    #matrix = pd.DataFrame({'Similarity Score': sim_score,'Paragraph Numbers': pg_list, 'Count': word_count}, index = words)\n",
    "    matrix = matrix.drop_duplicates()\n",
    "\n",
    "    matrix =  matrix.sort_values(by=['Similarity Score'],ascending=False)\n",
    "    #print(\"For document: {}, path: {}, keyword: {}, Similarity Score Threshold: {}\".format(os.path.basename(path),path,query,score))\n",
    "    #print(matrix)\n",
    "    return matrix\n",
    "   # print('\\n')\n",
    "\n",
    "#FI & PN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd1c7a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = []\n",
    "final_df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bbb022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check unigram,bigram, web app interface, check time complexity with 300 pages dataset, similar to google search from 1-100(access index of dataframe list: click 1 will trigger i-1 index of list)\n",
    "\n",
    "for i in all_path[:]:\n",
    "    output = sim_word(i,\"profit margin\",0.6)\n",
    "    word_df.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f2ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = [df for df in word_df if not df.empty] # removing dataframes that are empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b8c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows with Count having a value of 0\n",
    "for df in word_df:\n",
    "    df = df[df.Count != 0]\n",
    "    final_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa2293",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e8ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
