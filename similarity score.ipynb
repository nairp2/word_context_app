{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1adfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1bbb022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg',disable = ['ner', 'parser'])\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45878ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"Supervised learning is the machine learning task of \n",
    "         learning a function that maps an input to an output based \n",
    "         on example input-output pairs [1] It infers a function \n",
    "         from labeled training data consisting of a set of \n",
    "         training examples [2] In supervised learning, each \n",
    "         example is a pair consisting of an input object \n",
    "         (typically a vector) and a desired output value (also \n",
    "         called the supervisory signal). A supervised learning \n",
    "         algorithm analyzes the training data and produces an \n",
    "         inferred function, which can be used for mapping new \n",
    "         examples. An optimal scenario will allow for the algorithm \n",
    "         to correctly determine the class labels for unseen \n",
    "         instances. This requires the learning algorithm to  \n",
    "         generalize from the training data to unseen situations \n",
    "         in a 'reasonable' way (see inductive bias)?\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1afaf0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Supervised learning is the machine learning task of \\n         learning a function that maps an input to an output based \\n         on example input-output pairs [1] It infers a function \\n         from labeled training data consisting of a set of \\n         training examples [2] In supervised learning, each \\n         example is a pair consisting of an input object \\n         (typically a vector) and a desired output value (also \\n         called the supervisory signal). A supervised learning \\n         algorithm analyzes the training data and produces an \\n         inferred function, which can be used for mapping new \\n         examples. An optimal scenario will allow for the algorithm \\n         to correctly determine the class labels for unseen \\n         instances. This requires the learning algorithm to  \\n         generalize from the training data to unseen situations \\n         in a 'reasonable' way (see inductive bias)?\\n      \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b52aad7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[supervised,\n",
       " learning,\n",
       " machine,\n",
       " learn,\n",
       " task,\n",
       " learn,\n",
       " function,\n",
       " map,\n",
       " input,\n",
       " output,\n",
       " base,\n",
       " example,\n",
       " input,\n",
       " output,\n",
       " pair,\n",
       " 1,\n",
       " infer,\n",
       " function,\n",
       " label,\n",
       " training,\n",
       " datum,\n",
       " consist,\n",
       " set,\n",
       " training,\n",
       " example,\n",
       " 2,\n",
       " supervised,\n",
       " learning,\n",
       " example,\n",
       " pair,\n",
       " consist,\n",
       " input,\n",
       " object,\n",
       " typically,\n",
       " vector,\n",
       " desire,\n",
       " output,\n",
       " value,\n",
       " call,\n",
       " supervisory,\n",
       " signal,\n",
       " supervised,\n",
       " learn,\n",
       " algorithm,\n",
       " analyze,\n",
       " training,\n",
       " datum,\n",
       " produce,\n",
       " infer,\n",
       " function,\n",
       " map,\n",
       " new,\n",
       " example,\n",
       " optimal,\n",
       " scenario,\n",
       " allow,\n",
       " algorithm,\n",
       " correctly,\n",
       " determine,\n",
       " class,\n",
       " label,\n",
       " unseen,\n",
       " instance,\n",
       " require,\n",
       " learning,\n",
       " algorithm,\n",
       " generalize,\n",
       " training,\n",
       " datum,\n",
       " unseen,\n",
       " situation,\n",
       " reasonable,\n",
       " way,\n",
       " inductive,\n",
       " bias]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = re.sub('[\\W]+', '', text.lower())\n",
    "    return text\n",
    "\n",
    "\n",
    "tokens = nlp(doc)\n",
    "lemma_list = []\n",
    "for token in tokens:\n",
    "    if token.is_stop is False:\n",
    "        token_preprocessed = preprocessor(token.lemma_)\n",
    "        if token_preprocessed != '':\n",
    "             lemma_list.append(nlp(token_preprocessed))\n",
    "lemma_list\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbdcf96",
   "metadata": {},
   "source": [
    "### Trying with spacy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2f50761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:learn \n",
      "Word Found:learning \n",
      "Similarity score:0.77\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:learn \n",
      "Similarity score:1.00\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:learn \n",
      "Similarity score:1.00\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:training \n",
      "Similarity score:0.54\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:training \n",
      "Similarity score:0.54\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:learning \n",
      "Similarity score:0.77\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:learn \n",
      "Similarity score:1.00\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:training \n",
      "Similarity score:0.54\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:learning \n",
      "Similarity score:0.77\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:training \n",
      "Similarity score:0.54\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:way \n",
      "Similarity score:0.60\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "key = nlp(\"learn\")\n",
    "\n",
    "for i in lemma_list:\n",
    "    s = key.similarity(i)\n",
    "    \n",
    "    if s > 0.5:\n",
    "        print(\"Key:{} \\nWord Found:{} \\nSimilarity score:{:0.2f}\".format(key,i,s))\n",
    "        print(\"----------------------------------------------------\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f171a36d",
   "metadata": {},
   "source": [
    "### Trying with transformer models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc6e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd8b1ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('stsb-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62b8471c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: Word\n",
      "Sentence 2: Word\n",
      "Similarity score: 1.0000007152557373\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"Word\"\n",
    "sentence2 = \"Word\"\n",
    "# encode sentences to get their embeddings\n",
    "embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "# compute similarity scores of two embeddings\n",
    "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "print(\"Sentence 1:\", sentence1)\n",
    "print(\"Sentence 2:\", sentence2)\n",
    "print(\"Similarity score:\", cosine_scores.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c1eae41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:learn \n",
      "Word Found:learning \n",
      "Similarity score:0.94\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:learn \n",
      "Similarity score:1.00\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:learn \n",
      "Similarity score:1.00\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:training \n",
      "Similarity score:0.52\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:training \n",
      "Similarity score:0.52\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:learning \n",
      "Similarity score:0.94\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:learn \n",
      "Similarity score:1.00\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:training \n",
      "Similarity score:0.52\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:class \n",
      "Similarity score:0.51\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:learning \n",
      "Similarity score:0.94\n",
      "----------------------------------------------------\n",
      "Key:learn \n",
      "Word Found:training \n",
      "Similarity score:0.52\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "key = \"learn\"\n",
    "\n",
    "embedding1 = model.encode(key, convert_to_tensor=True)\n",
    "\n",
    "for i in lemma_list:\n",
    "    i = i.text\n",
    "     \n",
    "    embedding2 = model.encode(i, convert_to_tensor=True)\n",
    "    # compute similarity scores of two embeddings\n",
    "    cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "    \n",
    "    if cosine_scores > 0.5:\n",
    "        print(\"Key:{} \\nWord Found:{} \\nSimilarity score:{:0.2f}\".format(key,i,np.array(cosine_scores)[0][0]))\n",
    "        print(\"----------------------------------------------------\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "608daeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17314279"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea5e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
