{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1adfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "import os\n",
    "import docx2txt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option(\"max_rows\", None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1bbb022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg',disable = ['ner', 'parser'])\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2cf89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fishrak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fishrak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61155840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642e39da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcd56b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbc_doc\\business\\001.docx\n",
      "bbc_doc\\business\\002.docx\n",
      "bbc_doc\\business\\003.docx\n",
      "bbc_doc\\business\\004.docx\n",
      "bbc_doc\\business\\005.docx\n",
      "bbc_doc\\business\\~$001.docx\n",
      "bbc_doc\\entertainment\\001.docx\n",
      "bbc_doc\\entertainment\\002.docx\n",
      "bbc_doc\\entertainment\\003.docx\n",
      "bbc_doc\\entertainment\\004.docx\n",
      "bbc_doc\\entertainment\\005.docx\n",
      "bbc_doc\\politics\\001.docx\n",
      "bbc_doc\\politics\\002.docx\n",
      "bbc_doc\\politics\\003.docx\n",
      "bbc_doc\\politics\\004.docx\n",
      "bbc_doc\\politics\\005.docx\n",
      "bbc_doc\\sports\\001.docx\n",
      "bbc_doc\\sports\\002.docx\n",
      "bbc_doc\\sports\\003.docx\n",
      "bbc_doc\\sports\\004.docx\n",
      "bbc_doc\\sports\\005.docx\n",
      "bbc_doc\\technology\\001.docx\n",
      "bbc_doc\\technology\\002.docx\n",
      "bbc_doc\\technology\\003.docx\n",
      "bbc_doc\\technology\\004.docx\n",
      "bbc_doc\\technology\\005.docx\n"
     ]
    }
   ],
   "source": [
    "def all_file_paths(master_directory):\n",
    "\n",
    "    path_l = []\n",
    "    for root, dirs, files in os.walk(master_directory):\n",
    "\n",
    "\n",
    "\n",
    "        if files:\n",
    "            for i in files:\n",
    "                path = os.path.join(root, i)\n",
    "                path = path.replace(os.path.sep, '/')\n",
    "\n",
    "                if path == 'bbc_doc/desktop.ini':\n",
    "                    continue\n",
    "                path_l.append(path)\n",
    "\n",
    "\n",
    "    return(path_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59ed012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = re.sub('[\\W]+', '', text.lower())\n",
    "    return text\n",
    "\n",
    "\n",
    "def findline(word,doc_list): #Added doc_list as var\n",
    "    line_num = []\n",
    "    line_list = []\n",
    "    for i in range(len(doc_list)):\n",
    "        if word in doc_list[i]:\n",
    "            line_num.append(i+1)\n",
    "            line_list.append(doc_list[i])\n",
    "\n",
    "    return line_num, line_list\n",
    "\n",
    "\n",
    "def word_occurences(text, tokens_str, query):\n",
    "    \n",
    "    wordfreq = []\n",
    "    for w in tokens_str:\n",
    "        wordfreq.append(tokens_str.count(w))\n",
    "\n",
    "\n",
    "    for i in list(zip(tokens_str, wordfreq)):\n",
    "        if query == i[0]:\n",
    "            word_occurences = (i[0], i[1])\n",
    "            break\n",
    "\n",
    "    line = 0\n",
    "    word_not_found = []\n",
    "\n",
    "    for word in text:\n",
    "        if word == '\\n':\n",
    "            line += 1\n",
    "\n",
    "    doc_list = text.lower().splitlines()\n",
    "    doc_list = [i for i in doc_list if i]\n",
    "    \n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46639d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f8f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_word(path,query,score):\n",
    " \n",
    "#FI\n",
    "    text = docx2txt.process(path)\n",
    "    string_txt = str(text)\n",
    "\n",
    "    tokens = nlp(text)\n",
    "    lemma_list = []\n",
    "    for token in tokens:\n",
    "        if token.is_stop is False:\n",
    "            token_preprocessed = preprocessor(token.text)\n",
    "            if token_preprocessed != '':\n",
    "                 lemma_list.append(nlp(token_preprocessed))\n",
    "   \n",
    "\n",
    "    tokens_str = text.lower()\n",
    "    tokens_str = word_tokenize(tokens_str)\n",
    "    tokens_str = [i for i in tokens_str if not i in stop_words]    \n",
    "#FI    \n",
    "    \n",
    "    query = query\n",
    "    \n",
    "    l_q = len(nlp(query))\n",
    "    \n",
    "    if l_q >= 2:\n",
    "        \n",
    "        bi_g = []\n",
    "        c=0\n",
    "        while c < len(lemma_list)-1:\n",
    "\n",
    "            bi_g.append(nlp(lemma_list[c].text+\" \"+lemma_list[c+1].text))\n",
    "            c = c+1\n",
    "\n",
    "\n",
    "        lemma_list = lemma_list+bi_g\n",
    "    \n",
    "#PN \n",
    "    doc_list = word_occurences(text, lemma_list, query)\n",
    "    \n",
    "    string_list = []\n",
    "    paragraph_list = []\n",
    "    paragraph_matrix_list = []\n",
    "    \n",
    "    string_list.append(string_txt)\n",
    "    \n",
    "    string_list = string_list[0].splitlines()\n",
    "    \n",
    "    for word in string_list:\n",
    "        if word != '':\n",
    "            paragraph_list.append(word)\n",
    "    \n",
    "    #print(paragraph_list)\n",
    "#PN\n",
    "\n",
    "\n",
    "#FI & PN\n",
    "    key = nlp(query)\n",
    "    \n",
    "    sim_score = []\n",
    "    words = []\n",
    "    word_count = []\n",
    "    #paragraph_word_set = set()\n",
    "   # paragraph_sentences = []\n",
    "\n",
    "    pg_list = []\n",
    "    line_list_words = []\n",
    "\n",
    "    for i in lemma_list:\n",
    "        s = key.similarity(i)\n",
    "\n",
    "        if s > score:\n",
    "            words.append(i)\n",
    "                \n",
    "            # PN\n",
    "            \n",
    "\n",
    "            word_found_list = []\n",
    "            word_found_list.append(str(i.text))\n",
    "            word_found_lines, line_list = findline(word_found_list[0],doc_list)\n",
    "            #print(line_list)\n",
    "            l_line_list = []\n",
    "            l_line_list.append(tuple(line_list))\n",
    "            paragraph_lines = list(line_list)\n",
    "            #paragraph_lines = [ele for ele in paragraph_lines if ele != []]\n",
    "            word_count.append(len(word_found_lines))\n",
    "            \n",
    "            pg_list.append(tuple(word_found_lines))\n",
    "            line_list_words.append(tuple(line_list +\"\\n\"))\n",
    "            \n",
    "            sim_score.append(s)\n",
    "    \n",
    "    matrix = pd.DataFrame({'Similarity Score': sim_score,'Paragraph Numbers': pg_list, 'Count': word_count, 'Paragraphs': line_list_words, 'Path': path}, index = words)\n",
    "\n",
    "    #matrix = pd.DataFrame({'Similarity Score': sim_score,'Paragraph Numbers': pg_list, 'Count': word_count}, index = words)\n",
    "    matrix = matrix.drop_duplicates()\n",
    "\n",
    "    matrix =  matrix.sort_values(by=['Similarity Score'],ascending=False)\n",
    "    #print(\"For document: {}, path: {}, keyword: {}, Similarity Score Threshold: {}\".format(os.path.basename(path),path,query,score))\n",
    "    #print(matrix)\n",
    "    return matrix\n",
    "   # print('\\n')\n",
    "\n",
    "#FI & PN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91571f3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_df = []\n",
    "final_df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e315557d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15120/3857806146.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_path\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msim_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"profit\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mword_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15120/2236867528.py\u001b[0m in \u001b[0;36msim_word\u001b[1;34m(path, query, score)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mpg_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_found_lines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0mline_list_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline_list\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0msim_score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
     ]
    }
   ],
   "source": [
    "# Check unigram,bigram, web app interface, check time complexity with 300 pages dataset, similar to google search from 1-100(access index of dataframe list: click 1 will trigger i-1 index of list)\n",
    "\n",
    "for i in all_path[:]:\n",
    "    output = sim_word(i,\"profit\",0.6)\n",
    "    word_df.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3565ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = [df for df in word_df if not df.empty] # removing dataframes that are empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0543d3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows with Count having a value of 0\n",
    "for df in word_df:\n",
    "    df = df[df.Count != 0]\n",
    "    final_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe75a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c47f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966f1e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [\"at1\",\"at2\",\"at3\",\"at4\"]\n",
    "l2 = []\n",
    "l3 = []\n",
    "l4 = []\n",
    "\n",
    "i=0\n",
    "while i < len(l)-1:\n",
    "    print(i)\n",
    "    l2.append(l[i]+\" \"+l[i+1])\n",
    "    i = i+1\n",
    "    \n",
    "i=0\n",
    "while i < len(l)-2:\n",
    "    print(i)\n",
    "    l3.append(l[i]+\" \"+l[i+1]+\" \"+l[i+2])\n",
    "    i = i+1\n",
    "\n",
    "i=0\n",
    "while i < len(l)-3:\n",
    "    print(i)\n",
    "    l4.append(l[i]+\" \"+l[i+1]+\" \"+l[i+2]+\" \"+l[i+3])\n",
    "    i = i+1\n",
    "    \n",
    "l + l2 + l3 + l4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b1a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [(\"ad sales boost time warner profit\",\"quarterly profits at us media giant timewarner jumped 76% to  1.13𝑏𝑛(£600𝑚)𝑓𝑜𝑟𝑡ℎ𝑒𝑡ℎ𝑟𝑒𝑒𝑚𝑜𝑛𝑡ℎ𝑠𝑡𝑜𝑑𝑒𝑐𝑒𝑚𝑏𝑒𝑟,𝑓𝑟𝑜𝑚 639m year-earlier.\")]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if word_found_lines:\n",
    "#     pass\n",
    "# else:\n",
    "#     continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f171a36d",
   "metadata": {},
   "source": [
    "### Trying with transformer models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import numpy as np\n",
    "\n",
    "# model = SentenceTransformer('stsb-roberta-large')\n",
    "\n",
    "# sentence1 = \"Word\"\n",
    "# sentence2 = \"Word\"\n",
    "# # encode sentences to get their embeddings\n",
    "# embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "# embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "# # compute similarity scores of two embeddings\n",
    "# cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "# print(\"Sentence 1:\", sentence1)\n",
    "# print(\"Sentence 2:\", sentence2)\n",
    "# print(\"Similarity score:\", cosine_scores.item())\n",
    "\n",
    "\n",
    "# key = \"learn\"\n",
    "\n",
    "# embedding1 = model.encode(key, convert_to_tensor=True)\n",
    "\n",
    "# for i in lemma_list:\n",
    "#     i = i.text\n",
    "     \n",
    "#     embedding2 = model.encode(i, convert_to_tensor=True)\n",
    "#     # compute similarity scores of two embeddings\n",
    "#     cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "    \n",
    "#     if cosine_scores > 0.5:\n",
    "#         print(\"Key:{} \\nWord Found:{} \\nSimilarity score:{:0.2f}\".format(key,i,np.array(cosine_scores)[0][0]))\n",
    "#         print(\"----------------------------------------------------\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8b1ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sim_word(path,query,score=0.6):\n",
    " \n",
    "# #FI\n",
    "#     text = docx2txt.process(path)\n",
    "#     string_txt = str(text)\n",
    "\n",
    "\n",
    "#     tokens = nlp(text)\n",
    "#     lemma_list = []\n",
    "#     for token in tokens:\n",
    "#         if token.is_stop is False:\n",
    "#             token_preprocessed = preprocessor(token.text)\n",
    "#             if token_preprocessed != '':\n",
    "#                  lemma_list.append(nlp(token_preprocessed))\n",
    "   \n",
    "\n",
    "# #     tokens_str = text.lower()\n",
    "# #     tokens_str = word_tokenize(tokens_str)\n",
    "# #     tokens_str = [i for i in tokens_str if not i in stop_words]  \n",
    "    \n",
    "\n",
    "    \n",
    "#     l_q = len(nlp(query))\n",
    "    \n",
    "#     if l_q >= 2:\n",
    "        \n",
    "#         bi_g = []\n",
    "#         c=0\n",
    "#         while c < len(lemma_list)-1:\n",
    "\n",
    "#             bi_g.append(nlp(lemma_list[c].text+\" \"+lemma_list[c+1].text))\n",
    "#             c = c+1\n",
    "\n",
    "\n",
    "#         lemma_list = lemma_list+bi_g\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# #PN    \n",
    "#     doc_list = word_occurences(text, lemma_list, query)\n",
    "    \n",
    "#     string_list = []\n",
    "#     paragraph_list = []\n",
    "#     paragraph_matrix_list = []\n",
    "    \n",
    "#     string_list.append(string_txt)\n",
    "    \n",
    "#     string_list = string_list[0].splitlines()\n",
    "    \n",
    "#     for word in string_list:\n",
    "#         if word != '':\n",
    "#             paragraph_list.append(word)\n",
    "\n",
    "# #PN\n",
    "\n",
    "\n",
    "# #FI & PN\n",
    "#     key = nlp(query)\n",
    "    \n",
    "#     sim_score = []\n",
    "#     words = []\n",
    "#     word_count = []\n",
    "\n",
    "#     pg_list = []\n",
    "#     line_list_words = []\n",
    "\n",
    "\n",
    "#     for i in lemma_list:\n",
    "#         s = key.similarity(i)\n",
    "\n",
    "#         if s > score:\n",
    "#             words.append(i)\n",
    "                \n",
    "#             # PN\n",
    "#             sim_score.append(s)\n",
    "            \n",
    "#             word_found_list = []\n",
    "#             word_found_list.append(str(i.text))\n",
    "#             word_found_lines,line_list = findline(word_found_list[0],doc_list)\n",
    "            \n",
    "#             if word_found_lines:\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 continue\n",
    "\n",
    "#             l_line_list = []\n",
    "#             l_line_list.append(tuple(line_list))\n",
    "#             paragraph_lines = list(line_list)\n",
    "#             #paragraph_lines = [ele for ele in paragraph_lines if ele != []]\n",
    "#             word_count.append(len(word_found_lines))\n",
    "            \n",
    "#             pg_list.append(tuple(word_found_lines))\n",
    "#             words.append(i)\n",
    "#             sim_score.append(s)\n",
    "            \n",
    "\n",
    "#     matrix = pd.DataFrame({'Similarity Score': sim_score,'Paragraph Numbers': pg_list, 'Count': word_count, 'Paragraphs': line_list_words, 'Path': path}, index = words)\n",
    "\n",
    "#     #matrix = pd.DataFrame({'Similarity Score': sim_score,'Paragraph Numbers': pg_list, 'Count': word_count}, index = words)\n",
    "#     matrix = matrix.drop_duplicates()\n",
    "\n",
    "#     matrix =  matrix.sort_values(by=['Similarity Score'],ascending=False)\n",
    "#     #print(\"For document: {}, path: {}, keyword: {}, Similarity Score Threshold: {}\".format(os.path.basename(path),path,query,score))\n",
    "#     #print(matrix)\n",
    "#     return matrix\n",
    "\n",
    "# #FI & PN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8471c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1eae41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
