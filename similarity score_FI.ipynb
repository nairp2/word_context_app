{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1adfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "import os\n",
    "import docx2txt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1bbb022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg',disable = ['ner', 'parser'])\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2cf89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pnair\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pnair\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61155840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642e39da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcd56b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbc_doc\\business\\001.docx\n",
      "bbc_doc\\business\\002.docx\n",
      "bbc_doc\\business\\003.docx\n",
      "bbc_doc\\business\\004.docx\n",
      "bbc_doc\\business\\005.docx\n",
      "bbc_doc\\entertainment\\001.docx\n",
      "bbc_doc\\entertainment\\002.docx\n",
      "bbc_doc\\entertainment\\003.docx\n",
      "bbc_doc\\entertainment\\004.docx\n",
      "bbc_doc\\entertainment\\005.docx\n",
      "bbc_doc\\politics\\001.docx\n",
      "bbc_doc\\politics\\002.docx\n",
      "bbc_doc\\politics\\003.docx\n",
      "bbc_doc\\politics\\004.docx\n",
      "bbc_doc\\politics\\005.docx\n",
      "bbc_doc\\sports\\001.docx\n",
      "bbc_doc\\sports\\002.docx\n",
      "bbc_doc\\sports\\003.docx\n",
      "bbc_doc\\sports\\004.docx\n",
      "bbc_doc\\sports\\005.docx\n",
      "bbc_doc\\technology\\001.docx\n",
      "bbc_doc\\technology\\002.docx\n",
      "bbc_doc\\technology\\003.docx\n",
      "bbc_doc\\technology\\004.docx\n",
      "bbc_doc\\technology\\005.docx\n"
     ]
    }
   ],
   "source": [
    "def all_file_paths(master_directory):   \n",
    "    business_txt_list = []\n",
    "    path_l = []\n",
    "    for root, dirs, files in os.walk(master_directory):\n",
    "        \n",
    "        \n",
    "\n",
    "        if files:\n",
    "            for i in files:\n",
    "                path = \"{}\\{}\".format(root,i)\n",
    "                if path == 'bbc_doc\\desktop.ini':\n",
    "                    continue\n",
    "                path_l.append(path)\n",
    "                \n",
    "    \n",
    "    return(path_l)\n",
    "\n",
    "all_path = all_file_paths('bbc_doc')\n",
    "\n",
    "for i in all_path:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59ed012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = re.sub('[\\W]+', '', text.lower())\n",
    "    return text\n",
    "\n",
    "\n",
    "def findline(word,doc_list): #Added doc_list as var\n",
    "    line_num = []\n",
    "    for i in range(len(doc_list)):\n",
    "        if word in doc_list[i]:\n",
    "            line_num.append(i+1)\n",
    "\n",
    "    return line_num\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46639d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f8f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_word(path,query,score=0.6):\n",
    " \n",
    "#FI\n",
    "    text = docx2txt.process(path)\n",
    "\n",
    "\n",
    "    tokens = nlp(text)\n",
    "    lemma_list = []\n",
    "    for token in tokens:\n",
    "        if token.is_stop is False:\n",
    "            token_preprocessed = preprocessor(token.text)\n",
    "            if token_preprocessed != '':\n",
    "                 lemma_list.append(nlp(token_preprocessed))\n",
    "   \n",
    "\n",
    "#     tokens_str = text.lower()\n",
    "#     tokens_str = word_tokenize(tokens_str)\n",
    "#     tokens_str = [i for i in tokens_str if not i in stop_words]  \n",
    "    \n",
    "\n",
    "    \n",
    "    l_q = len(nlp(query))\n",
    "    \n",
    "    if l_q >= 2:\n",
    "        \n",
    "        bi_g = []\n",
    "        c=0\n",
    "        while c < len(lemma_list)-1:\n",
    "\n",
    "            bi_g.append(nlp(lemma_list[c].text+\" \"+lemma_list[c+1].text))\n",
    "            c = c+1\n",
    "\n",
    "\n",
    "        lemma_list = lemma_list+bi_g\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#PN    \n",
    "    wordfreq = []\n",
    "    for w in lemma_list:\n",
    "        wordfreq.append(lemma_list.count(w))\n",
    "\n",
    "\n",
    "    for i in list(zip(lemma_list, wordfreq)):\n",
    "        if query == i[0]:\n",
    "            word_occurences = (i[0], i[1])\n",
    "            break\n",
    "\n",
    "    line = 0\n",
    "    word_not_found = []\n",
    "\n",
    "    for word in text:\n",
    "        if word == '\\n':\n",
    "            line += 1\n",
    "\n",
    "    doc_list = text.lower().splitlines()\n",
    "    doc_list = [i for i in doc_list if i]\n",
    "\n",
    "#PN\n",
    "\n",
    "\n",
    "#FI & PN\n",
    "    key = nlp(query)\n",
    "    \n",
    "    sim_score = []\n",
    "    words = []\n",
    "\n",
    "    pg_list = []\n",
    "\n",
    "    for i in lemma_list:\n",
    "        s = key.similarity(i)\n",
    "\n",
    "        if s > score:\n",
    "\n",
    "            word_found_list = []\n",
    "            word_found_list.append(str(i.text))\n",
    "            word_found_lines = findline(word_found_list[0],doc_list)\n",
    "            \n",
    "            if word_found_lines:\n",
    "                pass\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "\n",
    "            pg_list.append(tuple(word_found_lines))\n",
    "            words.append(i)\n",
    "            sim_score.append(s)\n",
    "            \n",
    "\n",
    "    print(words)\n",
    "            \n",
    "    matrix = pd.DataFrame({'Similarity Score': sim_score,'paragraph': pg_list}, index = list(words))\n",
    "    matrix = matrix.drop_duplicates()\n",
    "\n",
    "    matrix =  matrix.sort_values(by=['Similarity Score'],ascending=False)\n",
    "\n",
    "    print(\"For document: {}, path: {}, keyword: {}, Similarity Score Threshold: {}\".format(os.path.basename(path),path,query,score))\n",
    "    print(matrix)\n",
    "    print('\\n')\n",
    "\n",
    "#FI & PN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91571f3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sales, profit, profits, sales, sales, sales, profits, gains, profit, profits, profit, revenues, profits, profits, profit, revenues, earnings, revenue, profit, pay, revenue]\n",
      "For document: 001.docx, path: bbc_doc\\business\\001.docx, keyword: profit, Similarity Score Threshold: 0.6\n",
      "            Similarity Score        paragraph\n",
      "(profit)            1.000000  (1, 2, 3, 4, 5)\n",
      "(profits)           0.913009     (2, 3, 4, 5)\n",
      "(revenue)           0.791138        (4, 5, 6)\n",
      "(earnings)          0.762458             (5,)\n",
      "(revenues)          0.735938           (4, 5)\n",
      "(sales)             0.627998           (1, 3)\n",
      "(gains)             0.620257             (3,)\n",
      "(pay)               0.602320             (6,)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in all_path[:1]:\n",
    "    sim_word(i,\"profit\",0.6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "966f1e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['at1',\n",
       " 'at2',\n",
       " 'at3',\n",
       " 'at4',\n",
       " 'at1 at2',\n",
       " 'at2 at3',\n",
       " 'at3 at4',\n",
       " 'at1 at2 at3',\n",
       " 'at2 at3 at4',\n",
       " 'at1 at2 at3 at4']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [\"at1\",\"at2\",\"at3\",\"at4\"]\n",
    "l2 = []\n",
    "l3 = []\n",
    "l4 = []\n",
    "\n",
    "i=0\n",
    "while i < len(l)-1:\n",
    "    print(i)\n",
    "    l2.append(l[i]+\" \"+l[i+1])\n",
    "    i = i+1\n",
    "    \n",
    "i=0\n",
    "while i < len(l)-2:\n",
    "    print(i)\n",
    "    l3.append(l[i]+\" \"+l[i+1]+\" \"+l[i+2])\n",
    "    i = i+1\n",
    "\n",
    "i=0\n",
    "while i < len(l)-3:\n",
    "    print(i)\n",
    "    l4.append(l[i]+\" \"+l[i+1]+\" \"+l[i+2]+\" \"+l[i+3])\n",
    "    i = i+1\n",
    "    \n",
    "l + l2 + l3 + l4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f171a36d",
   "metadata": {},
   "source": [
    "### Trying with transformer models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bc6e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import numpy as np\n",
    "\n",
    "# model = SentenceTransformer('stsb-roberta-large')\n",
    "\n",
    "# sentence1 = \"Word\"\n",
    "# sentence2 = \"Word\"\n",
    "# # encode sentences to get their embeddings\n",
    "# embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "# embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "# # compute similarity scores of two embeddings\n",
    "# cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "# print(\"Sentence 1:\", sentence1)\n",
    "# print(\"Sentence 2:\", sentence2)\n",
    "# print(\"Similarity score:\", cosine_scores.item())\n",
    "\n",
    "\n",
    "# key = \"learn\"\n",
    "\n",
    "# embedding1 = model.encode(key, convert_to_tensor=True)\n",
    "\n",
    "# for i in lemma_list:\n",
    "#     i = i.text\n",
    "     \n",
    "#     embedding2 = model.encode(i, convert_to_tensor=True)\n",
    "#     # compute similarity scores of two embeddings\n",
    "#     cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "    \n",
    "#     if cosine_scores > 0.5:\n",
    "#         print(\"Key:{} \\nWord Found:{} \\nSimilarity score:{:0.2f}\".format(key,i,np.array(cosine_scores)[0][0]))\n",
    "#         print(\"----------------------------------------------------\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8b1ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8471c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1eae41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
